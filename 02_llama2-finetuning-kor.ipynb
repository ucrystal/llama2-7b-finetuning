{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03ebca3-618e-4491-b0d3-871de32961da",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Step 0: 허깅페이스 토큰 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db8e6a28-5323-4a93-bce3-e86dc5bfb584",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee0c687f2ca460a8b43a5e378829f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec554c9-e4a1-4305-b61a-02f6accbe0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Step 1: 모델 로드하기\n",
    "###모델 weight 폴더가 있는 위치로 model_id 설정. 예제에서는 허깅페이스 repo id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae0eb0c5-0606-41c4-a44e-86891bf737e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "902983e2-1356-484d-9049-76efb5263bea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf4b69474c4402cb56dfde1db6bcfea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "model_id=\"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model =LlamaForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf62ac9-dc32-43fc-aa2b-dda047305cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Step 1-1: 베이스 모델 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b976f61a-41d1-4481-81bf-c037e5830945",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarize this dialog:\n",
      "A: 안녕, Tom. 내일 오후 바쁘세요?\n",
      "B: 아마도 바쁠 것 같아. 무슨 일이야?\n",
      "A: 동물 보호소에 나갈 수 있을까요?\n",
      "B: 뭘 하려고 해?\n",
      "A: 아들을 위해 강아지를 얻고 싶어요.\n",
      "B: 그럼 그를 정말 행복하게 만들 거야.\n",
      "A: 네, 우리는 그 일에 대해 많이 논의했어요. 이제 그가 준비됐다고 생각해요.\n",
      "B: 그게 좋아. 강아지를 키우는 것은 힘들어. 아기 키우는 것처럼 ;-)\n",
      "A: 그럼 작은 강아지를 사줄게요.\n",
      "B: 크게 자라지 않을 거지 ;-)\n",
      "A: 그리고 너무 많이 먹지 않을 거야 ;-)\n",
      "B: 그가 어떤 걸 좋아할지 알아?\n",
      "A: 네, 그를 지난 월요일 거기 데려갔어요. 그가 정말 좋아하는 하나를 보여줬어요.\n",
      "B: 그래서 그를 데려가야 했겠지.\n",
      "A: 그는 곧바로 집에 데려가고 싶었어요 ;-)\n",
      "B: 그럼 그 강아지 이름이 뭐가 될지 궁금하네.\n",
      "A: 그는 그의 죽은 햄스터 이름인 Lemmy로 지을 거래요. 그는 Motorhead 팬이라서 :-)))\n",
      "---\n",
      "Summary:\n",
      "A: 안녕, Tom. 내일 오후 바쁘세요?\n",
      "B: 아마도 바쁠 것 같아. 무슨 일이야?\n",
      "A: 동물 보호소에 나갈 수 있을까요?\n",
      "B: 뭘 하�\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = \"\"\"\n",
    "Summarize this dialog:\n",
    "A: 안녕, Tom. 내일 오후 바쁘세요?\n",
    "B: 아마도 바쁠 것 같아. 무슨 일이야?\n",
    "A: 동물 보호소에 나갈 수 있을까요?\n",
    "B: 뭘 하려고 해?\n",
    "A: 아들을 위해 강아지를 얻고 싶어요.\n",
    "B: 그럼 그를 정말 행복하게 만들 거야.\n",
    "A: 네, 우리는 그 일에 대해 많이 논의했어요. 이제 그가 준비됐다고 생각해요.\n",
    "B: 그게 좋아. 강아지를 키우는 것은 힘들어. 아기 키우는 것처럼 ;-)\n",
    "A: 그럼 작은 강아지를 사줄게요.\n",
    "B: 크게 자라지 않을 거지 ;-)\n",
    "A: 그리고 너무 많이 먹지 않을 거야 ;-)\n",
    "B: 그가 어떤 걸 좋아할지 알아?\n",
    "A: 네, 그를 지난 월요일 거기 데려갔어요. 그가 정말 좋아하는 하나를 보여줬어요.\n",
    "B: 그래서 그를 데려가야 했겠지.\n",
    "A: 그는 곧바로 집에 데려가고 싶었어요 ;-)\n",
    "B: 그럼 그 강아지 이름이 뭐가 될지 궁금하네.\n",
    "A: 그는 그의 죽은 햄스터 이름인 Lemmy로 지을 거래요. 그는 Motorhead 팬이라서 :-)))\n",
    "---\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02125df1-8396-487d-aafa-4750cc683f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Step 2: 한국어 데이터셋 로드 및 전처리 (프롬프트 적용 & 토큰화)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "730a8387-fd31-4f1c-a4c2-d34aa57738ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c10fabfe82944ef8e42ef18b423bdd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/282 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336ce471068a40f3ac4c437dbccd26c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/282 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf443bb50d4a4afe8b0dd9d983249cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Concat:   0%|          | 0/282 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "## 1. read input dataset: 데이터셋 불러오기 & 테스트 데이터 선정\n",
    "df = pd.read_json('sample/dialog_summary_kor.json')\n",
    "dataset = Dataset.from_pandas(df)\n",
    "input_dataset = dataset.train_test_split(test_size=0.98, seed=24)['train']\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "## 2. apply prompt: 학습에 사용할 프롬프트 정의 및 학습 데이터셋을 해당 포맷으로 정리\n",
    "prompt = (\n",
    "    f\"Summarize this dialog:\\n{{dialog}}\\n---\\nSummary:\\n\"\n",
    ")\n",
    "\n",
    "def apply_prompt_template(sample):\n",
    "    return {\n",
    "        \"prompt\": prompt.format(dialog=sample[\"dialog\"]),\n",
    "        \"summary\": sample[\"summary\"],\n",
    "    }\n",
    "\n",
    "prompt_dataset = input_dataset.map(apply_prompt_template, remove_columns=list(input_dataset.features))\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "## 3. tokenize: LLM 모델은 자연어 문장의 토큰 시퀀스를 입력받으므로, 의미있는 단위로 문장을 나눈 토큰화 작업을 수행함\n",
    "def tokenize_add_label(sample):\n",
    "    prompt = tokenizer.encode(tokenizer.bos_token + sample[\"prompt\"], add_special_tokens=False)\n",
    "    summary = tokenizer.encode(sample[\"summary\"] +  tokenizer.eos_token, add_special_tokens=False)\n",
    "\n",
    "    sample = {\n",
    "        \"input_ids\": prompt + summary,\n",
    "        \"attention_mask\" : [1] * (len(prompt) + len(summary)),\n",
    "        \"labels\": [-100] * len(prompt) + summary,\n",
    "        }\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "tokenize_dataset = prompt_dataset.map(tokenize_add_label, remove_columns=list(prompt_dataset.features))\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "## 4. concat dataset (chunking)\n",
    "#전처리된 데이터셋을 청크 단위로 묶는 과정\n",
    "#현재 모든 LLM은 입력 토큰들끼리 얼마나 관계가 있는지 계산해야하는데(attention) 입력이 늘어날수록 컴퓨팅 처리량 급증함. 입력 사이즈 제한 필수\n",
    "chunk_size=2048\n",
    "buffer = {\n",
    "    \"input_ids\": [],\n",
    "    \"attention_mask\": [],\n",
    "    \"labels\": [],\n",
    "    }\n",
    "\n",
    "samples = []\n",
    "\n",
    "for sample in tqdm(tokenize_dataset, desc=\"Concat\", unit=' examples'):\n",
    "    buffer = {k: v + sample[k] for k,v in buffer.items()}\n",
    "    \n",
    "    while len(next(iter(buffer.values()))) > chunk_size:\n",
    "        samples.append({k: v[:chunk_size] for k,v in buffer.items()})\n",
    "        buffer = {k: v[chunk_size:] for k,v in buffer.items()}\n",
    "\n",
    "concat_dataset = Dataset.from_list(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20037579-18e7-48da-a632-699e1ea42fbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 282\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9db5dfe-ce79-4e4b-8aaa-586844dffaa2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 68\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3cf39b4-df97-49ec-b9e1-433a0238f6ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Summarize this dialog:\n",
      "Fred: 안녕 친구야! 너한테 뭔가 말해줘야 해! 우리 아이가 생긴 거야\n",
      "Mark: 진짜야?\n",
      "Fred: 응, 확실해. 이미 우리 부모님한테도 말했어\n",
      "Mark: 젠장! 진짜로?\n",
      "Fred: 곧 모두가 알게 될 거라서 제일 먼저 내 친한 친구한테 말하려고 했어\n",
      "Mark: 그래. 그럼 알게 된 순간 어땠어?\n",
      "Fred: 사실 말하자면 두려움과 불안함을 느꼈어\n",
      "Mark: 알겠다, 친구야, 알겠다\n",
      "Fred: 그런데 몇 주 지나니까 다 괜찮아졌어. 사실 아이가 빨리 태어날 줄은 몰랐어\n",
      "Mark: 그래? 몇 주만에 다 괜찮아진 거야?!\n",
      "Fred: 응, 우리가 원했던 거라서 그런 거야. 그냥 이렇게 빨리 될 줄은 몰랐어\n",
      "Mark: 알겠다. 너의 인생이 완전히 바뀌게 될 거야, 친구야!\n",
      "Fred: 알아\n",
      "Mark: 큰 결심이야! 나는 아직 준비가 안 돼, 너무 이기적이라서\n",
      "Fred: 사람들은 그게 모두 가치가 있다고 말해. 게다가 우리 가족들이 지원해줄 거야\n",
      "Mark: 너는 많은 지원이 필요할 거야\n",
      "Fred: 긍정적으로 생각할 거야\n",
      "Mark: 그래, 그냥 새로운 쓰레기 같은 삶에 대해 투덜거리지 말아줘! 하하!\n",
      "Fred: 우리를 위해 기뻐해줘서 고마워!\n",
      "Mark: 미안하지만, 친구야! 그래서 아기는 언제 예정이야?\n",
      "Fred: 약 5개월 후에야\n",
      "Mark: 미안해! 정말로 너를 위해 기뻐. 그냥 내 친한 친구를 잃고 싶지 않아서 그래\n",
      "\n",
      "---\n",
      "Summary:\n",
      " Fred는 아기를 가질 예정이고, 그를 기다릴 수 없다.</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenize_dataset[0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9d7b064-cf1c-49d6-837f-ebd0d628ae58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Summarize this dialog:\n",
      "Fred: 안녕 친구야! 너한테 뭔가 말해줘야 해! 우리 아이가 생긴 거야\n",
      "Mark: 진짜야?\n",
      "Fred: 응, 확실해. 이미 우리 부모님한테도 말했어\n",
      "Mark: 젠장! 진짜로?\n",
      "Fred: 곧 모두가 알게 될 거라서 제일 먼저 내 친한 친구한테 말하려고 했어\n",
      "Mark: 그래. 그럼 알게 된 순간 어땠어?\n",
      "Fred: 사실 말하자면 두려움과 불안함을 느꼈어\n",
      "Mark: 알겠다, 친구야, 알겠다\n",
      "Fred: 그런데 몇 주 지나니까 다 괜찮아졌어. 사실 아이가 빨리 태어날 줄은 몰랐어\n",
      "Mark: 그래? 몇 주만에 다 괜찮아진 거야?!\n",
      "Fred: 응, 우리가 원했던 거라서 그런 거야. 그냥 이렇게 빨리 될 줄은 몰랐어\n",
      "Mark: 알겠다. 너의 인생이 완전히 바뀌게 될 거야, 친구야!\n",
      "Fred: 알아\n",
      "Mark: 큰 결심이야! 나는 아직 준비가 안 돼, 너무 이기적이라서\n",
      "Fred: 사람들은 그게 모두 가치가 있다고 말해. 게다가 우리 가족들이 지원해줄 거야\n",
      "Mark: 너는 많은 지원이 필요할 거야\n",
      "Fred: 긍정적으로 생각할 거야\n",
      "Mark: 그래, 그냥 새로운 쓰레기 같은 삶에 대해 투덜거리지 말아줘! 하하!\n",
      "Fred: 우리를 위해 기뻐해줘서 고마워!\n",
      "Mark: 미안하지만, 친구야! 그래서 아기는 언제 예정이야?\n",
      "Fred: 약 5개월 후에야\n",
      "Mark: 미안해! 정말로 너를 위해 기뻐. 그냥 내 친한 친구를 잃고 싶지 않아서 그래\n",
      "\n",
      "---\n",
      "Summary:\n",
      " Fred는 아기를 가질 예정이고, 그를 기다릴 수 없다.</s><s>Summarize this dialog:\n",
      "제니: 안녕! 네가 떠난 이후로 동네가 너무 따분해!\n",
      "메간: 아아아! 나도 여기서 적응하기 힘들어...\n",
      "제니: 정말? 그럼 다시 돌아와... 네가 있을 때는 너무 활기찼는데 이제는 아무도 나오지 않아!!!\n",
      "메간: 그럼 너희들이 만남을 계획해봐. 나도 함께 할게...\n",
      "제니: 좋겠다. 내가 여러 번 말했는데도 그들은 \"메간만 할 수 있어\"라고 해...\n",
      "메간: 아아 너희들이 나를 그리워하니까 기분 좋다... 나도 너희들을 너무 보고 싶어...\n",
      "제니: 그럼 만남을 계획해볼래?\n",
      "메간: 그래 알려줄게\n",
      "제니: 좋아\n",
      "메간: 좋은 하루 보내!\n",
      "제니: 너도 그래.\n",
      "\n",
      "---\n",
      "Summary:\n",
      " 메간은 제니의 동네에서 이사를 왔다. 모두 서로를 그리워한다. 메간은 만남을 계획할 것이다.</s><s>Summarize this dialog:\n",
      "앤드류: 오, 페이스북에 돌아왔구나!\n",
      "앤드류: !\n",
      "미아: 잠깐 돌아왔어\n",
      "미아: 어떤 그룹에 가입되기를 기다리고 있어서 뭔가 팔고 싶어서\n",
      "미아: 페이스북은 못생겼어\n",
      "앤드류: <파일_이모티콘>\n",
      "앤드류: 아니야\n",
      "앤드류: 최고의 여우 스티커가 있어서 그렇지\n",
      "미아: 그게 유일한 좋은 부분이야\n",
      "앤드류: 그게 최고의 부분이야 :$\n",
      "앤드류: 뭐 팔려고 해?\n",
      "미아: 침대를 팔려고 해\n",
      "앤드류: :O\n",
      "앤드류: 크리스마스에 새로 사온 거야?\n",
      "미아: 하하, 아니야\n",
      "미아: 새로운 건 오래 전에 샀어\n",
      "미아: 옛날 침대는 아직 여기에 있어\n",
      "앤드류: 알\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(concat_dataset[0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc58931-2043-4d16-b8a1-eb0b2452a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Step3. 모델 PEFT 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe446184-29a5-4acf-9cb5-53aba7a3fa1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "def create_peft_config(model):\n",
    "    from peft import (\n",
    "        get_peft_model,\n",
    "        LoraConfig,\n",
    "        TaskType,\n",
    "        prepare_model_for_int8_training,\n",
    "    )\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules = [\"q_proj\", \"v_proj\"]\n",
    "    )\n",
    "\n",
    "    # prepare int-8 model for training\n",
    "    model = prepare_model_for_int8_training(model)\n",
    "    # model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model, peft_config\n",
    "\n",
    "# create peft config\n",
    "model, lora_config = create_peft_config(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4a2b9c-18b5-4277-b516-059a78aa975f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Step4. Profiler옵션 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbe7bbf2-a6a5-4b8e-9221-90a3b68585fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "from contextlib import nullcontext\n",
    "enable_profiler = False\n",
    "output_dir = \"tmp/llama-output-kor\"\n",
    "\n",
    "config = {\n",
    "    'lora_config': lora_config,\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_train_epochs': 1,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'per_device_train_batch_size': 2,\n",
    "    'gradient_checkpointing': False,\n",
    "}\n",
    "\n",
    "# Set up profiler\n",
    "if enable_profiler:\n",
    "    wait, warmup, active, repeat = 1, 1, 2, 1\n",
    "    total_steps = (wait + warmup + active) * (1 + repeat)\n",
    "    schedule =  torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=repeat)\n",
    "    profiler = torch.profiler.profile(\n",
    "        schedule=schedule,\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(f\"{output_dir}/logs/tensorboard\"),\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True)\n",
    "    \n",
    "    class ProfilerCallback(TrainerCallback):\n",
    "        def __init__(self, profiler):\n",
    "            self.profiler = profiler\n",
    "            \n",
    "        def on_step_end(self, *args, **kwargs):\n",
    "            self.profiler.step()\n",
    "\n",
    "    profiler_callback = ProfilerCallback(profiler)\n",
    "else:\n",
    "    profiler = nullcontext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948302d1-3d2a-4f8a-b3c1-1486d7bb420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Step5. 파인튜닝 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80151e1c-c205-436c-b769-8ef8ac7f4a16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msjsj7226\u001b[0m (\u001b[33mdataplatform\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/0110seminar/wandb/run-20240109_013441-lmln7aiq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dataplatform/huggingface/runs/lmln7aiq' target=\"_blank\">fine-violet-8</a></strong> to <a href='https://wandb.ai/dataplatform/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dataplatform/huggingface' target=\"_blank\">https://wandb.ai/dataplatform/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dataplatform/huggingface/runs/lmln7aiq' target=\"_blank\">https://wandb.ai/dataplatform/huggingface/runs/lmln7aiq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 04:25, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.775800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define training args\n",
    "from transformers import default_data_collator, Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    bf16=True,  # Use BF16 if available\n",
    "    # logging strategies\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    max_steps=total_steps if enable_profiler else -1,\n",
    "    **{k:v for k,v in config.items() if k != 'lora_config'}\n",
    ")\n",
    "\n",
    "with profiler:\n",
    "    # Create Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model.to(\"cuda\"),\n",
    "        args=training_args,\n",
    "        train_dataset=concat_dataset,\n",
    "        data_collator=default_data_collator,\n",
    "        callbacks=[profiler_callback] if enable_profiler else [],\n",
    "        \n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00b5f73-4519-4d83-a31d-8b427b372fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Step6. 파인튜닝한 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66fa1ee7-df0c-4453-ac22-bf590cc9fe41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarize this dialog:\n",
      "A: 안녕, Tom. 내일 오후 바쁘세요?\n",
      "B: 아마도 바쁠 것 같아. 무슨 일이야?\n",
      "A: 동물 보호소에 나갈 수 있을까요?\n",
      "B: 뭘 하려고 해?\n",
      "A: 아들을 위해 강아지를 얻고 싶어요.\n",
      "B: 그럼 그를 정말 행복하게 만들 거야.\n",
      "A: 네, 우리는 그 일에 대해 많이 논의했어요. 이제 그가 준비됐다고 생각해요.\n",
      "B: 그게 좋아. 강아지를 키우는 것은 힘들어. 아기 키우는 것처럼 ;-)\n",
      "A: 그럼 작은 강아지를 사줄게요.\n",
      "B: 크게 자라지 않을 거지 ;-)\n",
      "A: 그리고 너무 많이 먹지 않을 거야 ;-)\n",
      "B: 그가 어떤 걸 좋아할지 알아?\n",
      "A: 네, 그를 지난 월요일 거기 데려갔어요. 그가 정말 좋아하는 하나를 보여줬어요.\n",
      "B: 그래서 그를 데려가야 했겠지.\n",
      "A: 그는 곧바로 집에 데려가고 싶었어요 ;-)\n",
      "B: 그럼 그 강아지 이름이 뭐가 될지 궁금하네.\n",
      "A: 그는 그의 죽은 햄스터 이름인 Lemmy로 지을 거래요. 그는 Motorhead 팬이라서 :-)))\n",
      "---\n",
      "Summary:\n",
      "A는 친구 B를 만나고 싶어하고 그녀가 동물 보호소에 가는 것을 돕기로 한다. 그녀는 아들에게 강아지를 얻기 위해 동물 보호소에 가고 싶다. 그녀는 많이 먹지 않고 좋아하는 것을 보여줄 거라고 한다. 그녀는 그녀의 아들에게 강아지를 데려가기 위해 집에 가기로 한다. 그녀는 그녀의 아들에게 그의 강아지를 죽은 햄스터의 이름인 Lemmy로 지을 거라고 한다.\n",
      "\n",
      "1. 아들이 Lemmy라는 강아지를 얻었다.\n",
      "2. 그는 죽은 햄스\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = \"\"\"\n",
    "Summarize this dialog:\n",
    "A: 안녕, Tom. 내일 오후 바쁘세요?\n",
    "B: 아마도 바쁠 것 같아. 무슨 일이야?\n",
    "A: 동물 보호소에 나갈 수 있을까요?\n",
    "B: 뭘 하려고 해?\n",
    "A: 아들을 위해 강아지를 얻고 싶어요.\n",
    "B: 그럼 그를 정말 행복하게 만들 거야.\n",
    "A: 네, 우리는 그 일에 대해 많이 논의했어요. 이제 그가 준비됐다고 생각해요.\n",
    "B: 그게 좋아. 강아지를 키우는 것은 힘들어. 아기 키우는 것처럼 ;-)\n",
    "A: 그럼 작은 강아지를 사줄게요.\n",
    "B: 크게 자라지 않을 거지 ;-)\n",
    "A: 그리고 너무 많이 먹지 않을 거야 ;-)\n",
    "B: 그가 어떤 걸 좋아할지 알아?\n",
    "A: 네, 그를 지난 월요일 거기 데려갔어요. 그가 정말 좋아하는 하나를 보여줬어요.\n",
    "B: 그래서 그를 데려가야 했겠지.\n",
    "A: 그는 곧바로 집에 데려가고 싶었어요 ;-)\n",
    "B: 그럼 그 강아지 이름이 뭐가 될지 궁금하네.\n",
    "A: 그는 그의 죽은 햄스터 이름인 Lemmy로 지을 거래요. 그는 Motorhead 팬이라서 :-)))\n",
    "---\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=300)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ccba1e-0598-4a7e-9a37-6208d4af34e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Step 7: 로컬에  모델  저장\n",
    "###Save model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e65d2d5e-4a03-4fe1-a47a-b1f8d59e269e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9900843a-2006-4224-82f8-69e333240f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Step 8: 배치  테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "254e4283-577d-4ae4-98ad-c537d68a1dbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "619dba053fe64f2d8352361234cf60c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/282 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = dataset.train_test_split(test_size=0.1, seed=24)['train']\n",
    "prompt_dataset = input_dataset.map(apply_prompt_template, remove_columns=list(input_dataset.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7152ba19-4d6f-47f2-aca0-5142b3de8e0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['summary', 'prompt'],\n",
       "    num_rows: 282\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8684a4d-038b-4075-a2cf-26763fd436ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.0/250.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d7eaa12-3699-4730-af17-497f8c95c289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from time import gmtime, strftime\n",
    "\n",
    "\n",
    "def test_dialog_kor_model(sample):\n",
    "    start_time = time.time()\n",
    "    model_input = tokenizer(sample[\"prompt\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_result = tokenizer.decode(model.generate(**model_input, max_new_tokens=300)[0], skip_special_tokens=True)\n",
    "\n",
    "    running_time = (time.time() - start_time)\n",
    "\n",
    "    response = generated_result.replace(sample[\"prompt\"], \"\").lstrip()\n",
    "\n",
    "\n",
    "    result = {\n",
    "        \"prompt\": sample[\"prompt\"],\n",
    "        \"summary\": sample[\"summary\"],\n",
    "        \"response\": response,\n",
    "        \"running_time\": running_time\n",
    "        }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585a595d-d8d9-4057-bbb3-46e5d325a9e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 118/282 [40:12<58:05, 21.25s/it]  "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "list_result = []\n",
    "\n",
    "for cur_sample in tqdm(prompt_dataset):\n",
    "    cur_result = test_dialog_kor_model(cur_sample)\n",
    "    list_result.append(cur_result)\n",
    "\n",
    "result_df = pd.DataFrame(list_result)\n",
    "\n",
    "output_name = output_dir.split('/')[-1] + strftime(\"_%Y%m%d_%H%M%S\", gmtime())\n",
    "result_df.to_excel(f'{output_name}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456af83e-96ed-44d4-8d1b-b7a4e14327c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['running_time'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6bac31-c25b-4cdd-adae-c309988646a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
