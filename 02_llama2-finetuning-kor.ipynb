{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03ebca3-618e-4491-b0d3-871de32961da",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Step 0: 허깅페이스 토큰 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db8e6a28-5323-4a93-bce3-e86dc5bfb584",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee0c687f2ca460a8b43a5e378829f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec554c9-e4a1-4305-b61a-02f6accbe0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Step 1: 모델 로드하기\n",
    "###모델 weight 폴더가 있는 위치로 model_id 설정. 예제에서는 허깅페이스 repo id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae0eb0c5-0606-41c4-a44e-86891bf737e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "902983e2-1356-484d-9049-76efb5263bea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf4b69474c4402cb56dfde1db6bcfea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "model_id=\"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model =LlamaForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf62ac9-dc32-43fc-aa2b-dda047305cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Step 1-1: 베이스 모델 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b976f61a-41d1-4481-81bf-c037e5830945",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarize this dialog:\n",
      "A: 안녕, Tom. 내일 오후 바쁘세요?\n",
      "B: 아마도 바쁠 것 같아. 무슨 일이야?\n",
      "A: 동물 보호소에 나갈 수 있을까요?\n",
      "B: 뭘 하려고 해?\n",
      "A: 아들을 위해 강아지를 얻고 싶어요.\n",
      "B: 그럼 그를 정말 행복하게 만들 거야.\n",
      "A: 네, 우리는 그 일에 대해 많이 논의했어요. 이제 그가 준비됐다고 생각해요.\n",
      "B: 그게 좋아. 강아지를 키우는 것은 힘들어. 아기 키우는 것처럼 ;-)\n",
      "A: 그럼 작은 강아지를 사줄게요.\n",
      "B: 크게 자라지 않을 거지 ;-)\n",
      "A: 그리고 너무 많이 먹지 않을 거야 ;-)\n",
      "B: 그가 어떤 걸 좋아할지 알아?\n",
      "A: 네, 그를 지난 월요일 거기 데려갔어요. 그가 정말 좋아하는 하나를 보여줬어요.\n",
      "B: 그래서 그를 데려가야 했겠지.\n",
      "A: 그는 곧바로 집에 데려가고 싶었어요 ;-)\n",
      "B: 그럼 그 강아지 이름이 뭐가 될지 궁금하네.\n",
      "A: 그는 그의 죽은 햄스터 이름인 Lemmy로 지을 거래요. 그는 Motorhead 팬이라서 :-)))\n",
      "---\n",
      "Summary:\n",
      "A: 안녕, Tom. 내일 오후 바쁘세요?\n",
      "B: 아마도 바쁠 것 같아. 무슨 일이야?\n",
      "A: 동물 보호소에 나갈 수 있을까요?\n",
      "B: 뭘 하�\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = \"\"\"\n",
    "Summarize this dialog:\n",
    "A: 안녕, Tom. 내일 오후 바쁘세요?\n",
    "B: 아마도 바쁠 것 같아. 무슨 일이야?\n",
    "A: 동물 보호소에 나갈 수 있을까요?\n",
    "B: 뭘 하려고 해?\n",
    "A: 아들을 위해 강아지를 얻고 싶어요.\n",
    "B: 그럼 그를 정말 행복하게 만들 거야.\n",
    "A: 네, 우리는 그 일에 대해 많이 논의했어요. 이제 그가 준비됐다고 생각해요.\n",
    "B: 그게 좋아. 강아지를 키우는 것은 힘들어. 아기 키우는 것처럼 ;-)\n",
    "A: 그럼 작은 강아지를 사줄게요.\n",
    "B: 크게 자라지 않을 거지 ;-)\n",
    "A: 그리고 너무 많이 먹지 않을 거야 ;-)\n",
    "B: 그가 어떤 걸 좋아할지 알아?\n",
    "A: 네, 그를 지난 월요일 거기 데려갔어요. 그가 정말 좋아하는 하나를 보여줬어요.\n",
    "B: 그래서 그를 데려가야 했겠지.\n",
    "A: 그는 곧바로 집에 데려가고 싶었어요 ;-)\n",
    "B: 그럼 그 강아지 이름이 뭐가 될지 궁금하네.\n",
    "A: 그는 그의 죽은 햄스터 이름인 Lemmy로 지을 거래요. 그는 Motorhead 팬이라서 :-)))\n",
    "---\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02125df1-8396-487d-aafa-4750cc683f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Step 2: 한국어 데이터셋 로드 및 전처리 (프롬프트 적용 & 토큰화)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "730a8387-fd31-4f1c-a4c2-d34aa57738ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c10fabfe82944ef8e42ef18b423bdd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/282 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336ce471068a40f3ac4c437dbccd26c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/282 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf443bb50d4a4afe8b0dd9d983249cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Concat:   0%|          | 0/282 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "## 1. read input dataset: 데이터셋 불러오기 & 테스트 데이터 선정\n",
    "df = pd.read_json('sample/dialog_summary_kor.json')\n",
    "dataset = Dataset.from_pandas(df)\n",
    "input_dataset = dataset.train_test_split(test_size=0.98, seed=24)['train']\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "## 2. apply prompt: 학습에 사용할 프롬프트 정의 및 학습 데이터셋을 해당 포맷으로 정리\n",
    "prompt = (\n",
    "    f\"Summarize this dialog:\\n{{dialog}}\\n---\\nSummary:\\n\"\n",
    ")\n",
    "\n",
    "def apply_prompt_template(sample):\n",
    "    return {\n",
    "        \"prompt\": prompt.format(dialog=sample[\"dialog\"]),\n",
    "        \"summary\": sample[\"summary\"],\n",
    "    }\n",
    "\n",
    "prompt_dataset = input_dataset.map(apply_prompt_template, remove_columns=list(input_dataset.features))\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "## 3. tokenize: LLM 모델은 자연어 문장의 토큰 시퀀스를 입력받으므로, 의미있는 단위로 문장을 나눈 토큰화 작업을 수행함\n",
    "def tokenize_add_label(sample):\n",
    "    prompt = tokenizer.encode(tokenizer.bos_token + sample[\"prompt\"], add_special_tokens=False)\n",
    "    summary = tokenizer.encode(sample[\"summary\"] +  tokenizer.eos_token, add_special_tokens=False)\n",
    "\n",
    "    sample = {\n",
    "        \"input_ids\": prompt + summary,\n",
    "        \"attention_mask\" : [1] * (len(prompt) + len(summary)),\n",
    "        \"labels\": [-100] * len(prompt) + summary,\n",
    "        }\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "tokenize_dataset = prompt_dataset.map(tokenize_add_label, remove_columns=list(prompt_dataset.features))\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "## 4. concat dataset (chunking)\n",
    "#전처리된 데이터셋을 청크 단위로 묶는 과정\n",
    "#현재 모든 LLM은 입력 토큰들끼리 얼마나 관계가 있는지 계산해야하는데(attention) 입력이 늘어날수록 컴퓨팅 처리량 급증함. 입력 사이즈 제한 필수\n",
    "chunk_size=2048\n",
    "buffer = {\n",
    "    \"input_ids\": [],\n",
    "    \"attention_mask\": [],\n",
    "    \"labels\": [],\n",
    "    }\n",
    "\n",
    "samples = []\n",
    "\n",
    "for sample in tqdm(tokenize_dataset, desc=\"Concat\", unit=' examples'):\n",
    "    buffer = {k: v + sample[k] for k,v in buffer.items()}\n",
    "    \n",
    "    while len(next(iter(buffer.values()))) > chunk_size:\n",
    "        samples.append({k: v[:chunk_size] for k,v in buffer.items()})\n",
    "        buffer = {k: v[chunk_size:] for k,v in buffer.items()}\n",
    "\n",
    "concat_dataset = Dataset.from_list(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20037579-18e7-48da-a632-699e1ea42fbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 282\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9db5dfe-ce79-4e4b-8aaa-586844dffaa2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 68\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3cf39b4-df97-49ec-b9e1-433a0238f6ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Summarize this dialog:\n",
      "Fred: 안녕 친구야! 너한테 뭔가 말해줘야 해! 우리 아이가 생긴 거야\n",
      "Mark: 진짜야?\n",
      "Fred: 응, 확실해. 이미 우리 부모님한테도 말했어\n",
      "Mark: 젠장! 진짜로?\n",
      "Fred: 곧 모두가 알게 될 거라서 제일 먼저 내 친한 친구한테 말하려고 했어\n",
      "Mark: 그래. 그럼 알게 된 순간 어땠어?\n",
      "Fred: 사실 말하자면 두려움과 불안함을 느꼈어\n",
      "Mark: 알겠다, 친구야, 알겠다\n",
      "Fred: 그런데 몇 주 지나니까 다 괜찮아졌어. 사실 아이가 빨리 태어날 줄은 몰랐어\n",
      "Mark: 그래? 몇 주만에 다 괜찮아진 거야?!\n",
      "Fred: 응, 우리가 원했던 거라서 그런 거야. 그냥 이렇게 빨리 될 줄은 몰랐어\n",
      "Mark: 알겠다. 너의 인생이 완전히 바뀌게 될 거야, 친구야!\n",
      "Fred: 알아\n",
      "Mark: 큰 결심이야! 나는 아직 준비가 안 돼, 너무 이기적이라서\n",
      "Fred: 사람들은 그게 모두 가치가 있다고 말해. 게다가 우리 가족들이 지원해줄 거야\n",
      "Mark: 너는 많은 지원이 필요할 거야\n",
      "Fred: 긍정적으로 생각할 거야\n",
      "Mark: 그래, 그냥 새로운 쓰레기 같은 삶에 대해 투덜거리지 말아줘! 하하!\n",
      "Fred: 우리를 위해 기뻐해줘서 고마워!\n",
      "Mark: 미안하지만, 친구야! 그래서 아기는 언제 예정이야?\n",
      "Fred: 약 5개월 후에야\n",
      "Mark: 미안해! 정말로 너를 위해 기뻐. 그냥 내 친한 친구를 잃고 싶지 않아서 그래\n",
      "\n",
      "---\n",
      "Summary:\n",
      " Fred는 아기를 가질 예정이고, 그를 기다릴 수 없다.</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenize_dataset[0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9d7b064-cf1c-49d6-837f-ebd0d628ae58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Summarize this dialog:\n",
      "Fred: 안녕 친구야! 너한테 뭔가 말해줘야 해! 우리 아이가 생긴 거야\n",
      "Mark: 진짜야?\n",
      "Fred: 응, 확실해. 이미 우리 부모님한테도 말했어\n",
      "Mark: 젠장! 진짜로?\n",
      "Fred: 곧 모두가 알게 될 거라서 제일 먼저 내 친한 친구한테 말하려고 했어\n",
      "Mark: 그래. 그럼 알게 된 순간 어땠어?\n",
      "Fred: 사실 말하자면 두려움과 불안함을 느꼈어\n",
      "Mark: 알겠다, 친구야, 알겠다\n",
      "Fred: 그런데 몇 주 지나니까 다 괜찮아졌어. 사실 아이가 빨리 태어날 줄은 몰랐어\n",
      "Mark: 그래? 몇 주만에 다 괜찮아진 거야?!\n",
      "Fred: 응, 우리가 원했던 거라서 그런 거야. 그냥 이렇게 빨리 될 줄은 몰랐어\n",
      "Mark: 알겠다. 너의 인생이 완전히 바뀌게 될 거야, 친구야!\n",
      "Fred: 알아\n",
      "Mark: 큰 결심이야! 나는 아직 준비가 안 돼, 너무 이기적이라서\n",
      "Fred: 사람들은 그게 모두 가치가 있다고 말해. 게다가 우리 가족들이 지원해줄 거야\n",
      "Mark: 너는 많은 지원이 필요할 거야\n",
      "Fred: 긍정적으로 생각할 거야\n",
      "Mark: 그래, 그냥 새로운 쓰레기 같은 삶에 대해 투덜거리지 말아줘! 하하!\n",
      "Fred: 우리를 위해 기뻐해줘서 고마워!\n",
      "Mark: 미안하지만, 친구야! 그래서 아기는 언제 예정이야?\n",
      "Fred: 약 5개월 후에야\n",
      "Mark: 미안해! 정말로 너를 위해 기뻐. 그냥 내 친한 친구를 잃고 싶지 않아서 그래\n",
      "\n",
      "---\n",
      "Summary:\n",
      " Fred는 아기를 가질 예정이고, 그를 기다릴 수 없다.</s><s>Summarize this dialog:\n",
      "제니: 안녕! 네가 떠난 이후로 동네가 너무 따분해!\n",
      "메간: 아아아! 나도 여기서 적응하기 힘들어...\n",
      "제니: 정말? 그럼 다시 돌아와... 네가 있을 때는 너무 활기찼는데 이제는 아무도 나오지 않아!!!\n",
      "메간: 그럼 너희들이 만남을 계획해봐. 나도 함께 할게...\n",
      "제니: 좋겠다. 내가 여러 번 말했는데도 그들은 \"메간만 할 수 있어\"라고 해...\n",
      "메간: 아아 너희들이 나를 그리워하니까 기분 좋다... 나도 너희들을 너무 보고 싶어...\n",
      "제니: 그럼 만남을 계획해볼래?\n",
      "메간: 그래 알려줄게\n",
      "제니: 좋아\n",
      "메간: 좋은 하루 보내!\n",
      "제니: 너도 그래.\n",
      "\n",
      "---\n",
      "Summary:\n",
      " 메간은 제니의 동네에서 이사를 왔다. 모두 서로를 그리워한다. 메간은 만남을 계획할 것이다.</s><s>Summarize this dialog:\n",
      "앤드류: 오, 페이스북에 돌아왔구나!\n",
      "앤드류: !\n",
      "미아: 잠깐 돌아왔어\n",
      "미아: 어떤 그룹에 가입되기를 기다리고 있어서 뭔가 팔고 싶어서\n",
      "미아: 페이스북은 못생겼어\n",
      "앤드류: <파일_이모티콘>\n",
      "앤드류: 아니야\n",
      "앤드류: 최고의 여우 스티커가 있어서 그렇지\n",
      "미아: 그게 유일한 좋은 부분이야\n",
      "앤드류: 그게 최고의 부분이야 :$\n",
      "앤드류: 뭐 팔려고 해?\n",
      "미아: 침대를 팔려고 해\n",
      "앤드류: :O\n",
      "앤드류: 크리스마스에 새로 사온 거야?\n",
      "미아: 하하, 아니야\n",
      "미아: 새로운 건 오래 전에 샀어\n",
      "미아: 옛날 침대는 아직 여기에 있어\n",
      "앤드류: 알\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(concat_dataset[0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc58931-2043-4d16-b8a1-eb0b2452a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Step3. 모델 PEFT 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe446184-29a5-4acf-9cb5-53aba7a3fa1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "def create_peft_config(model):\n",
    "    from peft import (\n",
    "        get_peft_model,\n",
    "        LoraConfig,\n",
    "        TaskType,\n",
    "        prepare_model_for_int8_training,\n",
    "    )\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules = [\"q_proj\", \"v_proj\"]\n",
    "    )\n",
    "\n",
    "    # prepare int-8 model for training\n",
    "    model = prepare_model_for_int8_training(model)\n",
    "    # model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model, peft_config\n",
    "\n",
    "# create peft config\n",
    "model, lora_config = create_peft_config(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4a2b9c-18b5-4277-b516-059a78aa975f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Step4. Profiler옵션 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbe7bbf2-a6a5-4b8e-9221-90a3b68585fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "from contextlib import nullcontext\n",
    "enable_profiler = False\n",
    "output_dir = \"tmp/llama-output-kor\"\n",
    "\n",
    "# Set up profiler\n",
    "if enable_profiler:\n",
    "    wait, warmup, active, repeat = 1, 1, 2, 1\n",
    "    total_steps = (wait + warmup + active) * (1 + repeat)\n",
    "    schedule =  torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=repeat)\n",
    "    profiler = torch.profiler.profile(\n",
    "        schedule=schedule,\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(f\"{output_dir}/logs/tensorboard\"),\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True)\n",
    "    \n",
    "    class ProfilerCallback(TrainerCallback):\n",
    "        def __init__(self, profiler):\n",
    "            self.profiler = profiler\n",
    "            \n",
    "        def on_step_end(self, *args, **kwargs):\n",
    "            self.profiler.step()\n",
    "\n",
    "    profiler_callback = ProfilerCallback(profiler)\n",
    "else:\n",
    "    profiler = nullcontext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948302d1-3d2a-4f8a-b3c1-1486d7bb420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Step5. 파인튜닝 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80151e1c-c205-436c-b769-8ef8ac7f4a16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msjsj7226\u001b[0m (\u001b[33mdataplatform\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/0110seminar/wandb/run-20240109_013441-lmln7aiq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dataplatform/huggingface/runs/lmln7aiq' target=\"_blank\">fine-violet-8</a></strong> to <a href='https://wandb.ai/dataplatform/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dataplatform/huggingface' target=\"_blank\">https://wandb.ai/dataplatform/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dataplatform/huggingface/runs/lmln7aiq' target=\"_blank\">https://wandb.ai/dataplatform/huggingface/runs/lmln7aiq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 04:25, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.775800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import default_data_collator, Trainer, TrainingArguments\n",
    "\n",
    "config = {\n",
    "    'lora_config': lora_config,\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_train_epochs': 1,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'per_device_train_batch_size': 2,\n",
    "    'gradient_checkpointing': False,\n",
    "}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    bf16=True,  # Use BF16 if available\n",
    "    # logging strategies\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    max_steps=total_steps if enable_profiler else -1,\n",
    "    **{k:v for k,v in config.items() if k != 'lora_config'}\n",
    ")\n",
    "\n",
    "with profiler:\n",
    "    # Create Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model.to(\"cuda\"),\n",
    "        args=training_args,\n",
    "        train_dataset=concat_dataset,\n",
    "        data_collator=default_data_collator,\n",
    "        callbacks=[profiler_callback] if enable_profiler else [],\n",
    "        \n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00b5f73-4519-4d83-a31d-8b427b372fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Step6. 파인튜닝한 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66fa1ee7-df0c-4453-ac22-bf590cc9fe41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarize this dialog:\n",
      "A: 안녕, Tom. 내일 오후 바쁘세요?\n",
      "B: 아마도 바쁠 것 같아. 무슨 일이야?\n",
      "A: 동물 보호소에 나갈 수 있을까요?\n",
      "B: 뭘 하려고 해?\n",
      "A: 아들을 위해 강아지를 얻고 싶어요.\n",
      "B: 그럼 그를 정말 행복하게 만들 거야.\n",
      "A: 네, 우리는 그 일에 대해 많이 논의했어요. 이제 그가 준비됐다고 생각해요.\n",
      "B: 그게 좋아. 강아지를 키우는 것은 힘들어. 아기 키우는 것처럼 ;-)\n",
      "A: 그럼 작은 강아지를 사줄게요.\n",
      "B: 크게 자라지 않을 거지 ;-)\n",
      "A: 그리고 너무 많이 먹지 않을 거야 ;-)\n",
      "B: 그가 어떤 걸 좋아할지 알아?\n",
      "A: 네, 그를 지난 월요일 거기 데려갔어요. 그가 정말 좋아하는 하나를 보여줬어요.\n",
      "B: 그래서 그를 데려가야 했겠지.\n",
      "A: 그는 곧바로 집에 데려가고 싶었어요 ;-)\n",
      "B: 그럼 그 강아지 이름이 뭐가 될지 궁금하네.\n",
      "A: 그는 그의 죽은 햄스터 이름인 Lemmy로 지을 거래요. 그는 Motorhead 팬이라서 :-)))\n",
      "---\n",
      "Summary:\n",
      "A는 친구 B를 만나고 싶어하고 그녀가 동물 보호소에 가는 것을 돕기로 한다. 그녀는 아들에게 강아지를 얻기 위해 동물 보호소에 가고 싶다. 그녀는 많이 먹지 않고 좋아하는 것을 보여줄 거라고 한다. 그녀는 그녀의 아들에게 강아지를 데려가기 위해 집에 가기로 한다. 그녀는 그녀의 아들에게 그의 강아지를 죽은 햄스터의 이름인 Lemmy로 지을 거라고 한다.\n",
      "\n",
      "1. 아들이 Lemmy라는 강아지를 얻었다.\n",
      "2. 그는 죽은 햄스\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = \"\"\"\n",
    "Summarize this dialog:\n",
    "A: 안녕, Tom. 내일 오후 바쁘세요?\n",
    "B: 아마도 바쁠 것 같아. 무슨 일이야?\n",
    "A: 동물 보호소에 나갈 수 있을까요?\n",
    "B: 뭘 하려고 해?\n",
    "A: 아들을 위해 강아지를 얻고 싶어요.\n",
    "B: 그럼 그를 정말 행복하게 만들 거야.\n",
    "A: 네, 우리는 그 일에 대해 많이 논의했어요. 이제 그가 준비됐다고 생각해요.\n",
    "B: 그게 좋아. 강아지를 키우는 것은 힘들어. 아기 키우는 것처럼 ;-)\n",
    "A: 그럼 작은 강아지를 사줄게요.\n",
    "B: 크게 자라지 않을 거지 ;-)\n",
    "A: 그리고 너무 많이 먹지 않을 거야 ;-)\n",
    "B: 그가 어떤 걸 좋아할지 알아?\n",
    "A: 네, 그를 지난 월요일 거기 데려갔어요. 그가 정말 좋아하는 하나를 보여줬어요.\n",
    "B: 그래서 그를 데려가야 했겠지.\n",
    "A: 그는 곧바로 집에 데려가고 싶었어요 ;-)\n",
    "B: 그럼 그 강아지 이름이 뭐가 될지 궁금하네.\n",
    "A: 그는 그의 죽은 햄스터 이름인 Lemmy로 지을 거래요. 그는 Motorhead 팬이라서 :-)))\n",
    "---\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=300)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ccba1e-0598-4a7e-9a37-6208d4af34e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Step 7: 로컬에  모델  저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e65d2d5e-4a03-4fe1-a47a-b1f8d59e269e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9900843a-2006-4224-82f8-69e333240f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Step 8: 배치  테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "254e4283-577d-4ae4-98ad-c537d68a1dbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "619dba053fe64f2d8352361234cf60c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/282 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = dataset.train_test_split(test_size=0.1, seed=24)['train']\n",
    "prompt_dataset = input_dataset.map(apply_prompt_template, remove_columns=list(input_dataset.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7152ba19-4d6f-47f2-aca0-5142b3de8e0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['summary', 'prompt'],\n",
       "    num_rows: 282\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8684a4d-038b-4075-a2cf-26763fd436ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.0/250.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d7eaa12-3699-4730-af17-497f8c95c289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from time import gmtime, strftime\n",
    "\n",
    "\n",
    "def test_dialog_kor_model(sample):\n",
    "    start_time = time.time()\n",
    "    model_input = tokenizer(sample[\"prompt\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_result = tokenizer.decode(model.generate(**model_input, max_new_tokens=300)[0], skip_special_tokens=True)\n",
    "\n",
    "    running_time = (time.time() - start_time)\n",
    "\n",
    "    response = generated_result.replace(sample[\"prompt\"], \"\").lstrip()\n",
    "\n",
    "\n",
    "    result = {\n",
    "        \"prompt\": sample[\"prompt\"],\n",
    "        \"summary\": sample[\"summary\"],\n",
    "        \"response\": response,\n",
    "        \"running_time\": running_time\n",
    "        }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "585a595d-d8d9-4057-bbb3-46e5d325a9e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [1:47:08<00:00, 22.79s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "list_result = []\n",
    "\n",
    "for cur_sample in tqdm(prompt_dataset):\n",
    "    cur_result = test_dialog_kor_model(cur_sample)\n",
    "    list_result.append(cur_result)\n",
    "\n",
    "result_df = pd.DataFrame(list_result)\n",
    "\n",
    "output_name = output_dir.split('/')[-1] + strftime(\"_%Y%m%d_%H%M%S\", gmtime())\n",
    "result_df.to_excel(f'{output_name}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "456af83e-96ed-44d4-8d1b-b7a4e14327c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    282.000000\n",
       "mean      22.793217\n",
       "std       15.788848\n",
       "min        0.216467\n",
       "25%        9.597712\n",
       "50%       18.697583\n",
       "75%       33.756626\n",
       "max       52.180983\n",
       "Name: running_time, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df['running_time'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d6bac31-c25b-4cdd-adae-c309988646a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>summary</th>\n",
       "      <th>response</th>\n",
       "      <th>running_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summarize this dialog:\\nFred: 안녕 친구야! 너한테 뭔가 말...</td>\n",
       "      <td>Fred는 아기를 가질 예정이고, 그를 기다릴 수 없다.</td>\n",
       "      <td>Mark는 친구야! 너한테 뭔가 말해줄 거야! 우리 아이가 생긴 거야\\nFred는 ...</td>\n",
       "      <td>50.003531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Summarize this dialog:\\n제니: 안녕! 네가 떠난 이후로 동네가 ...</td>\n",
       "      <td>메간은 제니의 동네에서 이사를 왔다. 모두 서로를 그리워한다. 메간은 만남을 계획할...</td>\n",
       "      <td>제니는 메간이 여러 번 돌아왔으나 여러 사람들은 메간을 멀리 하고 있다. 메간은 제...</td>\n",
       "      <td>23.205800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Summarize this dialog:\\n앤드류: 오, 페이스북에 돌아왔구나!\\n...</td>\n",
       "      <td>미아는 페이스북 계정을 되찾아 침대를 판매하기 위해 페이스북 그룹 중 하나에 가입하...</td>\n",
       "      <td>앤드류는 페이스북에 돌아왔다. 그녀는 페이스북에 가입을 하고 있다. 그녀는 침대를 ...</td>\n",
       "      <td>30.135823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Summarize this dialog:\\nConrad: 얘야, 어디서 씨발놈이야?...</td>\n",
       "      <td>Frank가 회의를 까먹어서 Conrad가 화가 났다.</td>\n",
       "      <td>얘야, 어디서 씨발놈이야?\\n젠장, 까먹었어, 친구야\\n넌 멍청이야, 형제야\\n</td>\n",
       "      <td>12.660774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Summarize this dialog:\\nSean: &lt;다른 파일&gt;\\nChristi...</td>\n",
       "      <td>크리스틴은 독일인이에요. 그녀와 야콥은 방금 무의미한 싸움을 한 것 같아요.</td>\n",
       "      <td>* 저는 독일인이에요.\\n* 너는 말라얄람어, 타밀어, 힌디어, 칸나다어, 아랍어를...</td>\n",
       "      <td>19.921032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>Summarize this dialog:\\nSally: 안녕, 미안한데 월요일에 못...</td>\n",
       "      <td>Sally는 월요일에 Anna와 만날 수 없을 것이다. 왜냐하면 그녀의 엄마가 실신...</td>\n",
       "      <td>엄마가 병원에 입원했다. 12월에도 올 거야?\\n</td>\n",
       "      <td>6.633093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>Summarize this dialog:\\nPedro: 너무 답답해.\\nPedro:...</td>\n",
       "      <td>Pedro는 이미 TOEFL 시험을 5번이나 보았지만 좋은 점수를 받지 못했다. K...</td>\n",
       "      <td>컴퓨터로 시험을 보는 것은 엄청난 스트레스를 받아야 한다.\\n\\n</td>\n",
       "      <td>8.906242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>Summarize this dialog:\\n오스틴: 안녕, 내 차가 고장났어, 파티...</td>\n",
       "      <td>오스틴의 차가 고장나서 파티에 늦을 것 같다.</td>\n",
       "      <td>오스틴은 차가 고장났어, 파티에 늦을 것 같아 고민하고 있었다. 엠마는 도움이 필요...</td>\n",
       "      <td>31.768153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>Summarize this dialog:\\nFelix: 안녕, 줄리아에게 뭐 사려고...</td>\n",
       "      <td>아이든은 아마 줄리아에게 CD를 사줄 것 같아요. 마리아는 줄리아에게 생일 케이크를...</td>\n",
       "      <td>줄리아는 생일 케이크를 굽고 있다.\\n\\n---\\n</td>\n",
       "      <td>5.978415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>Summarize this dialog:\\nDax: 오늘 아빠가 치킨 윙을 만들 거...</td>\n",
       "      <td>마일스와 닥스는 오늘 밤 아빠의 치킨 윙을 먹을 예정이다.</td>\n",
       "      <td>Dax: 오늘 아빠가 치킨 윙을 만들 거야, 너도 오겠어?\\nMiles: 당연하지!...</td>\n",
       "      <td>16.114794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>282 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                prompt  \\\n",
       "0    Summarize this dialog:\\nFred: 안녕 친구야! 너한테 뭔가 말...   \n",
       "1    Summarize this dialog:\\n제니: 안녕! 네가 떠난 이후로 동네가 ...   \n",
       "2    Summarize this dialog:\\n앤드류: 오, 페이스북에 돌아왔구나!\\n...   \n",
       "3    Summarize this dialog:\\nConrad: 얘야, 어디서 씨발놈이야?...   \n",
       "4    Summarize this dialog:\\nSean: <다른 파일>\\nChristi...   \n",
       "..                                                 ...   \n",
       "277  Summarize this dialog:\\nSally: 안녕, 미안한데 월요일에 못...   \n",
       "278  Summarize this dialog:\\nPedro: 너무 답답해.\\nPedro:...   \n",
       "279  Summarize this dialog:\\n오스틴: 안녕, 내 차가 고장났어, 파티...   \n",
       "280  Summarize this dialog:\\nFelix: 안녕, 줄리아에게 뭐 사려고...   \n",
       "281  Summarize this dialog:\\nDax: 오늘 아빠가 치킨 윙을 만들 거...   \n",
       "\n",
       "                                               summary  \\\n",
       "0                      Fred는 아기를 가질 예정이고, 그를 기다릴 수 없다.   \n",
       "1    메간은 제니의 동네에서 이사를 왔다. 모두 서로를 그리워한다. 메간은 만남을 계획할...   \n",
       "2    미아는 페이스북 계정을 되찾아 침대를 판매하기 위해 페이스북 그룹 중 하나에 가입하...   \n",
       "3                       Frank가 회의를 까먹어서 Conrad가 화가 났다.   \n",
       "4           크리스틴은 독일인이에요. 그녀와 야콥은 방금 무의미한 싸움을 한 것 같아요.   \n",
       "..                                                 ...   \n",
       "277  Sally는 월요일에 Anna와 만날 수 없을 것이다. 왜냐하면 그녀의 엄마가 실신...   \n",
       "278  Pedro는 이미 TOEFL 시험을 5번이나 보았지만 좋은 점수를 받지 못했다. K...   \n",
       "279                          오스틴의 차가 고장나서 파티에 늦을 것 같다.   \n",
       "280  아이든은 아마 줄리아에게 CD를 사줄 것 같아요. 마리아는 줄리아에게 생일 케이크를...   \n",
       "281                   마일스와 닥스는 오늘 밤 아빠의 치킨 윙을 먹을 예정이다.   \n",
       "\n",
       "                                              response  running_time  \n",
       "0    Mark는 친구야! 너한테 뭔가 말해줄 거야! 우리 아이가 생긴 거야\\nFred는 ...     50.003531  \n",
       "1    제니는 메간이 여러 번 돌아왔으나 여러 사람들은 메간을 멀리 하고 있다. 메간은 제...     23.205800  \n",
       "2    앤드류는 페이스북에 돌아왔다. 그녀는 페이스북에 가입을 하고 있다. 그녀는 침대를 ...     30.135823  \n",
       "3         얘야, 어디서 씨발놈이야?\\n젠장, 까먹었어, 친구야\\n넌 멍청이야, 형제야\\n     12.660774  \n",
       "4    * 저는 독일인이에요.\\n* 너는 말라얄람어, 타밀어, 힌디어, 칸나다어, 아랍어를...     19.921032  \n",
       "..                                                 ...           ...  \n",
       "277                        엄마가 병원에 입원했다. 12월에도 올 거야?\\n      6.633093  \n",
       "278               컴퓨터로 시험을 보는 것은 엄청난 스트레스를 받아야 한다.\\n\\n      8.906242  \n",
       "279  오스틴은 차가 고장났어, 파티에 늦을 것 같아 고민하고 있었다. 엠마는 도움이 필요...     31.768153  \n",
       "280                       줄리아는 생일 케이크를 굽고 있다.\\n\\n---\\n      5.978415  \n",
       "281  Dax: 오늘 아빠가 치킨 윙을 만들 거야, 너도 오겠어?\\nMiles: 당연하지!...     16.114794  \n",
       "\n",
       "[282 rows x 4 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e17ca2-f018-414d-b7de-8c62e547dfaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
